{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "text1= \"The Transformer is a deep learning model introduced in 2017, used primarily in the field of natural language processing. Like recurrent neural networks, Transformers are designed to handle sequential data, such as natural language, for tasks such as translation and text summarization.\"\n",
    "text2 = \"Bidirectional Encoder Representations from Transformers is a technique for NLP pre-training developed by Google. BERT was created and published in 2018 by Jacob Devlin and his colleagues from Google. Google is leveraging BERT to better understand user searches.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the transform is a deep learn model introduc in 2017, use primarili in the field of natur languag processing. like recurr neural networks, transform are design to handl sequenti data, such as natur language, for task such as translat and text summarization.\n"
     ]
    }
   ],
   "source": [
    "#stemmming text1 using porter stemmer\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer=PorterStemmer()\n",
    "doc1=[stemmer.stem(token) for token in text1.split(\" \")]\n",
    "a=\" \".join(doc1)\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bidirect encod represent from transform is a techniqu for nlp pre-train develop by google. bert wa creat and publish in 2018 by jacob devlin and hi colleagu from google. googl is leverag bert to better understand user searches.\n"
     ]
    }
   ],
   "source": [
    "#stemming text2 using poter stemmer\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer=PorterStemmer()\n",
    "doc2=[stemmer.stem(token) for token in text2.split(\" \")]\n",
    "b=\" \".join(doc2)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vectorizing text from both the stemmed texts\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect1= CountVectorizer(binary=True)\n",
    "corpus1=[a,b]\n",
    "vect1.fit(corpus1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017:0\n",
      "2018:1\n",
      "and:2\n",
      "are:3\n",
      "as:4\n",
      "bert:5\n",
      "better:6\n",
      "bidirect:7\n",
      "by:8\n",
      "colleagu:9\n",
      "creat:10\n",
      "data:11\n",
      "deep:12\n",
      "design:13\n",
      "develop:14\n",
      "devlin:15\n",
      "encod:16\n",
      "field:17\n",
      "for:18\n",
      "from:19\n",
      "googl:20\n",
      "google:21\n",
      "handl:22\n",
      "hi:23\n",
      "in:24\n",
      "introduc:25\n",
      "is:26\n",
      "jacob:27\n",
      "languag:28\n",
      "language:29\n",
      "learn:30\n",
      "leverag:31\n",
      "like:32\n",
      "model:33\n",
      "natur:34\n",
      "networks:35\n",
      "neural:36\n",
      "nlp:37\n",
      "of:38\n",
      "pre:39\n",
      "primarili:40\n",
      "processing:41\n",
      "publish:42\n",
      "recurr:43\n",
      "represent:44\n",
      "searches:45\n",
      "sequenti:46\n",
      "such:47\n",
      "summarization:48\n",
      "task:49\n",
      "techniqu:50\n",
      "text:51\n",
      "the:52\n",
      "to:53\n",
      "train:54\n",
      "transform:55\n",
      "translat:56\n",
      "understand:57\n",
      "use:58\n",
      "user:59\n",
      "wa:60\n"
     ]
    }
   ],
   "source": [
    "#adding words to vocabulary & adding key for vocab\n",
    "vocab1=vect1.vocabulary_\n",
    "for key in sorted(vocab1.keys()):\n",
    "    print(\"{}:{}\".format(key, vocab1[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(vect1.transform(['Google published an article “Understanding searches better than ever before” and positioned BERT as one of its most important updates to the searching algorithms in recent years.']).toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.17928429]]\n"
     ]
    }
   ],
   "source": [
    "#finding cosine similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity = cosine_similarity(vect1.transform([a]).toarray(), vect1.transform([b]).toarray())\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vectorize using feature extraction\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect2= CountVectorizer(binary=True)\n",
    "corpus2=[text1,text2]\n",
    "vect2.fit(corpus2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017:0\n",
      "2018:1\n",
      "and:2\n",
      "are:3\n",
      "as:4\n",
      "bert:5\n",
      "better:6\n",
      "bidirectional:7\n",
      "by:8\n",
      "colleagues:9\n",
      "created:10\n",
      "data:11\n",
      "deep:12\n",
      "designed:13\n",
      "developed:14\n",
      "devlin:15\n",
      "encoder:16\n",
      "field:17\n",
      "for:18\n",
      "from:19\n",
      "google:20\n",
      "handle:21\n",
      "his:22\n",
      "in:23\n",
      "introduced:24\n",
      "is:25\n",
      "jacob:26\n",
      "language:27\n",
      "learning:28\n",
      "leveraging:29\n",
      "like:30\n",
      "model:31\n",
      "natural:32\n",
      "networks:33\n",
      "neural:34\n",
      "nlp:35\n",
      "of:36\n",
      "pre:37\n",
      "primarily:38\n",
      "processing:39\n",
      "published:40\n",
      "recurrent:41\n",
      "representations:42\n",
      "searches:43\n",
      "sequential:44\n",
      "such:45\n",
      "summarization:46\n",
      "tasks:47\n",
      "technique:48\n",
      "text:49\n",
      "the:50\n",
      "to:51\n",
      "training:52\n",
      "transformer:53\n",
      "transformers:54\n",
      "translation:55\n",
      "understand:56\n",
      "used:57\n",
      "user:58\n",
      "was:59\n"
     ]
    }
   ],
   "source": [
    "#establishing vocabulary\n",
    "vocab2=vect2.vocabulary_\n",
    "for key in sorted(vocab2.keys()):\n",
    "    print(\"{}:{}\".format(key, vocab2[key]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(vect2.transform(['Google published an article “Understanding searches better than ever before” and positioned BERT as one of its most important updates to the searching algorithms in recent years.']).toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.18215302]]\n"
     ]
    }
   ],
   "source": [
    "#establishing cosing similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity = cosine_similarity(vect2.transform([text1]).toarray(), vect2.transform([text2]).toarray())\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
